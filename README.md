# ğŸš€ Databricks Fraud Management Platform - One-Click Deployment

Complete infrastructure-as-code for deploying a production-ready fraud case management platform on Databricks.

## ğŸ¯ What This Deploys

This automated deployment creates a complete fraud investigation platform:

- âœ… **Databricks Workspace** with Unity Catalog enabled
- âœ… **Unity Catalog IAM Role** (self-assuming, ready for managed tables)
- âœ… **Lakebase Database Instance** (PostgreSQL-compatible)
- âœ… **Unity Catalog Objects** (catalogs, schemas, volumes)
- âœ… **Fraud Dashboard Tables** (Unity Catalog Delta tables with sample data)
- âœ… **SQL Warehouse** (for analytics and dashboard queries)
- âœ… **Users & Groups** with role-based access control
- âœ… **Fraud Case Management Application** (full-stack React + Node.js app)
- âœ… **Sample Data** (10 fraud cases, 3 investigators, transactions, indicators)

## ğŸ“‹ Prerequisites

### Required Tools
- [Terraform](https://www.terraform.io/downloads) (>= 1.5.0)
- [AWS CLI](https://aws.amazon.com/cli/) (configured with credentials)
- [Databricks CLI](https://docs.databricks.com/dev-tools/cli/index.html)
- [jq](https://stedolan.github.io/jq/) (JSON processor)
- [PostgreSQL Client](https://www.postgresql.org/download/) (`psql`)
- [Node.js](https://nodejs.org/) (>= 18.x)

### Required Credentials

1. **Databricks Service Principal** (Account Admin privileges)
   - Client ID
   - Client Secret

2. **AWS Account** with permissions to create:
   - VPC, Subnets, Security Groups
   - S3 Buckets
   - IAM Roles and Policies
   - Secrets Manager Secrets

3. **Databricks Account ID**

## ğŸ—ï¸ Project Structure

```
databricks-workspace-deployment/
â”œâ”€â”€ deploy-fraud-app.sh          # ğŸ¯ UNIFIED FRAUD APP DEPLOYMENT
â”œâ”€â”€ deploy-everything.sh         # ğŸš€ Infrastructure deployment only
â”œâ”€â”€ cleanup-everything.sh        # ğŸ§¹ Destroy all resources
â”œâ”€â”€ terraform.tfvars             # Configuration file (edit this!)
â”œâ”€â”€ main.tf                      # Main Terraform configuration
â”œâ”€â”€ variables.tf                 # Variable definitions
â”œâ”€â”€ outputs.tf                   # Output definitions
â”œâ”€â”€ sql/                         # SQL scripts for fraud tables
â”‚   â”œâ”€â”€ fraud_dashboard_schema.sql
â”‚   â””â”€â”€ fraud_dashboard_seed.sql
â””â”€â”€ modules/
    â”œâ”€â”€ users/                   # User & group management
    â”œâ”€â”€ unity-catalog/           # Catalog, schema, volume management
    â”œâ”€â”€ fraud-tables/            # Fraud dashboard tables
    â”œâ”€â”€ lakebase/                # Database instance management
    â””â”€â”€ apps/                    # Databricks Apps management

fraud-case-management/
â”œâ”€â”€ app.yaml                     # Databricks App configuration
â”œâ”€â”€ frontend/                    # React frontend
â”œâ”€â”€ backend/                     # Node.js/Express backend
â””â”€â”€ backend/db/                  # Database schema & seed data
```

## ğŸ¯ Quick Start (For Someone New)

**Everything is configured in ONE file:** `config.yaml`

```bash
# 1. Copy the example configuration
cp config.yaml.example config.yaml

# 2. Edit config.yaml with your settings
nano config.yaml

# 3. Deploy everything!
./deploy-everything.sh
```

That's it! The deployment script will automatically:
- Generate `terraform.tfvars` from your config
- Update `app.yaml` with correct values
- Create all necessary configuration files

---

## ğŸ¯ Unified Fraud App Deployment (Recommended)

The **simplest way** to deploy everything including the fraud case management app:

```bash
# Deploy everything with one command!
./deploy-fraud-app.sh
```

This script will:
1. âœ… Deploy all infrastructure (AWS + Databricks)
2. âœ… Create Unity Catalog with fraud dashboard tables
3. âœ… Create SQL Warehouse for analytics
4. âœ… Insert sample fraud data (10 cases, transactions, indicators)
5. âœ… Deploy the Fraud Case Management application
6. âœ… Provide you with the app URL

**Environment Variables:**
- `FRAUD_APP_DIR` - Path to fraud-case-management directory (default: `$HOME/fraud-case-management`)
- `DASHBOARD_ID` - Dashboard ID to reference (default: auto-detected)

**Example:**
```bash
FRAUD_APP_DIR=/path/to/fraud-case-management ./deploy-fraud-app.sh
```

**When to use this?**
- âœ… Fresh deployment from scratch
- âœ… You want everything set up automatically
- âœ… You're deploying the fraud management use case

**When NOT to use this?**
- âŒ You only want infrastructure without the app
- âŒ You're deploying a different application
- âŒ You want more control over each step

For infrastructure-only deployment, use `./deploy-everything.sh` instead.

---

## ğŸš€ DETAILED SETUP

### Step 1: Create Your Configuration File

Copy the example and edit with your settings:

```bash
cp config.yaml.example config.yaml
nano config.yaml
```

**Key settings to update in `config.yaml`:**
- `aws.region` - Your AWS region
- `aws.profile` - Your AWS CLI profile
- `databricks.account_id` - Your Databricks account ID
- `workspace.name` - Your desired workspace name
- `workspace.prefix` - Prefix for all AWS resources
- `unity_catalog.metastore_name` - Your UC metastore name
- `lakebase.instances[0].name` - Your database instance name
- `users.users` - Add your team members

### Step 2: Store Credentials in AWS Secrets Manager

```bash
# Store Databricks Service Principal credentials
aws secretsmanager create-secret \
  --name "databricks/service-principal" \
  --description "Databricks Service Principal for Terraform" \
  --secret-string '{
    "client_id": "YOUR_CLIENT_ID",
    "client_secret": "YOUR_CLIENT_SECRET",
    "account_id": "YOUR_ACCOUNT_ID"
  }' \
  --region eu-west-1 \
  --profile som

# Store OAuth credentials for workspace access
aws secretsmanager create-secret \
  --name "databricks/som-workspace/sp-oauth" \
  --description "Service Principal OAuth credentials" \
  --secret-string '{
    "client_id": "YOUR_CLIENT_ID",
    "client_secret": "YOUR_OAUTH_SECRET",
    "workspace_url": "https://your-workspace.cloud.databricks.com",
    "account_id": "YOUR_ACCOUNT_ID"
  }' \
  --region eu-west-1 \
  --profile som
```

### Step 3: Deploy Everything! ğŸš€

```bash
./deploy-everything.sh
```

**That's it!** â˜• The script will:
0. Generate all config files from `config.yaml`
1. Load credentials from AWS Secrets Manager
2. Deploy Databricks workspace with Unity Catalog
3. Create Lakebase database instance
4. Add users and groups
5. Create Unity Catalog objects
6. Setup PostgreSQL database schema
7. Deploy fraud case management application

Wait ~10-15 minutes for complete deployment.

---

## ğŸ“ Configuration Reference

All configuration is centralized in `config.yaml`. Here's what each section does:

### AWS Configuration
```yaml
aws:
  region: "eu-west-1"
  profile: "som"
```

### Databricks Configuration
```yaml
databricks:
  account_id: "your-account-id"
  service_principal:
    client_id: "your-sp-client-id"
```

### Workspace Configuration
```yaml
workspace:
  name: "my-workspace"
  prefix: "my-workspace"
  vpc:
    use_existing: true
    vpc_id: "vpc-xxxxx"
```

### Old Method (Still Works)

You can also edit `terraform.tfvars` directly if you prefer:

See `terraform.tfvars.example` for all available options.

### Step 4: Access Your Application

Once deployment completes, you'll see:

```
ğŸ‰ DEPLOYMENT COMPLETE! ğŸ‰

ğŸŒ Your Fraud Case Management Application:
   https://som-fraud-case-management-123456.aws.databricksapps.com

ğŸ“Š Resources Deployed:
   âœ… Databricks Workspace
   âœ… Unity Catalog Metastore
   âœ… Lakebase Instance
   âœ… Database with sample data
   âœ… Application (RUNNING)
```

Click the URL to access your fraud investigation platform!

## ğŸ§¹ Cleanup (Destroy Everything)

To remove all resources:

```bash
# Make the script executable
chmod +x cleanup-everything.sh

# Run cleanup
./cleanup-everything.sh
```

This will destroy:
- âŒ Databricks workspace
- âŒ Unity Catalog metastore
- âŒ Lakebase database instance
- âŒ All AWS resources (S3, IAM, etc.)
- âŒ Fraud case management application

## ğŸ”§ Manual Operations

### Update Infrastructure

Edit `terraform.tfvars` and apply changes:

```bash
terraform plan    # Preview changes
terraform apply   # Apply changes
```

### Add More Users

Edit `terraform.tfvars`:

```hcl
users = [
  {
    user_name    = "new.user@databricks.com"
    display_name = "New User"
    groups       = ["Data Engineers"]
  }
]
```

Then run:

```bash
terraform apply
```

### Redeploy Application Only

```bash
cd ../fraud-case-management
./deploy.sh som-fraud-case-management
```

### Connect to Lakebase Database

```bash
# Get database host from Terraform outputs
LAKEBASE_DNS=$(terraform output -json lakebase_database_instances | jq -r '."afc-lakebase-instance".read_write_dns')

# Connect with psql
PGSSLMODE=require PGPASSWORD="FraudAdmin2024!" psql \
  -h "$LAKEBASE_DNS" \
  -p 5432 \
  -U admin \
  -d fraud_detection_db
```

## ğŸ“¦ Terraform Modules

### Users Module (`modules/users/`)

Manages Databricks users and groups.

**Features:**
- Create users with display names
- Create groups
- Assign users to groups
- Support for built-in groups (e.g., `admins`)

### Unity Catalog Module (`modules/unity-catalog/`)

Manages Unity Catalog objects.

**Features:**
- Create catalogs
- Create schemas within catalogs
- Create volumes for unstructured data

### Lakebase Module (`modules/lakebase/`)

Manages Databricks Lakebase (PostgreSQL) instances.

**Features:**
- Create database instances with configurable capacity
- Enable PostgreSQL native login
- Register databases as Unity Catalog catalogs
- Automatic database creation

## ğŸ¯ Application Features

The deployed fraud case management application includes:

### For Fraud Investigators:
- ğŸ“Š **Dashboard** - Real-time fraud metrics and analytics
- ğŸ” **Case Management** - Investigate and manage fraud cases
- ğŸ“ˆ **Claims Analysis** - Analyze claims patterns
- ğŸš¨ **Alerts** - Real-time fraud alerts

### For Admins:
- ğŸ‘¥ **User Management** - Add/remove investigators
- ğŸ”§ **Configurations** - Configure thresholds and rules
- ğŸ“Š **Reporting** - Generate fraud reports

### Technical Stack:
- **Frontend**: React with modern UI components
- **Backend**: Node.js/Express REST API
- **Database**: PostgreSQL (Lakebase) with optimized schema
- **Auth**: Databricks Service Principal OAuth

## ğŸ” Security

### Credentials Management
- âœ… All secrets stored in AWS Secrets Manager
- âœ… Service Principal authentication
- âœ… SSL/TLS encrypted database connections
- âœ… Role-based access control

### Best Practices Implemented
- âœ… Infrastructure as Code (Terraform)
- âœ… Automated deployments
- âœ… Modular architecture
- âœ… Comprehensive logging
- âœ… Resource tagging

## ğŸ› Troubleshooting

### Terraform Errors

**Issue**: AWS resource limits
```bash
# Check your VPC limits
aws ec2 describe-account-attributes --region eu-west-1

# Use existing VPC
# Edit terraform.tfvars:
use_existing_vpc = true
existing_vpc_id = "vpc-xxxxx"
```

**Issue**: Databricks validation errors
```bash
# Wait 60 seconds for IAM propagation
sleep 60
terraform apply
```

### Database Connection Errors

**Issue**: Cannot connect to Lakebase
```bash
# Ensure PostgreSQL user exists in Databricks SQL Editor:
CREATE USER admin WITH PASSWORD 'FraudAdmin2024!';
GRANT ALL ON SCHEMA public TO admin;
```

### Application Deployment Errors

**Issue**: Databricks CLI not configured
```bash
# Reload credentials
cd ../fraud-case-management
source ./load-sp-oauth.sh
```

## ğŸ“š Additional Resources

- [Databricks Terraform Provider](https://registry.terraform.io/providers/databricks/databricks/latest/docs)
- [Databricks Apps Documentation](https://docs.databricks.com/dev-tools/databricks-apps/index.html)
- [Lakebase Documentation](https://docs.databricks.com/lakebase/index.html)
- [Unity Catalog Best Practices](https://docs.databricks.com/data-governance/unity-catalog/best-practices.html)

## ğŸ“ License

This project is for internal use only.

## ğŸ¤ Support

For issues or questions, contact: som.natarajan@databricks.com

---

**Built with â¤ï¸ for efficient fraud investigation**

